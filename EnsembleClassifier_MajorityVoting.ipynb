{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST on Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n",
      "(10000, 784)\n",
      "(50000,)\n",
      "(10000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "filename = 'mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, testing_data = pickle.load(f, encoding='latin1')\n",
    "\n",
    "train_data=np.array(training_data[0])\n",
    "train_target=np.array(training_data[1])\n",
    "valid_data=np.array(validation_data[0])\n",
    "valid_target=np.array(validation_data[1])\n",
    "test_data=np.array(testing_data[0])\n",
    "test_target=np.array(testing_data[1])\n",
    "\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "print(train_target.shape)\n",
    "print(valid_target.shape)\n",
    "print(test_target.shape)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load USPS on Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19999, 784)\n",
      "(19999,)\n"
     ]
    }
   ],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = 'USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)\n",
    "print(np.array(USPSMat).shape)\n",
    "print(np.array(USPSTar).shape)\n",
    "USPS_data=np.array(USPSMat)\n",
    "USPS_Target=np.array(USPSTar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax function\n",
    "\n",
    "def softmax(x):\n",
    "    #print(x.shape)\n",
    "    x -= np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding\n",
    "import scipy.sparse\n",
    "def oneHotEncoding(Y):\n",
    "    m = Y.shape[0]\n",
    "\n",
    "    oneHot = scipy.sparse.csr_matrix((np.ones(m), (Y, np.array(range(m)))))\n",
    "    oneHot = np.array(oneHot.todense()).T\n",
    "    return oneHot\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10)\n",
      "(10000, 10)\n",
      "(10000, 10)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "oneHotTestTarget=oneHotEncoding(test_target)\n",
    "oneHotValidTarget=oneHotEncoding(valid_target)\n",
    "oneHotTrainTarget=oneHotEncoding(train_target)\n",
    "print(oneHotTrainTarget.shape)\n",
    "print(oneHotValidTarget.shape)\n",
    "print(oneHotTestTarget.shape)\n",
    "print(train_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getGradientandLoss(w,x,y,lamda,bias):\n",
    "    m = x.shape[0] # number of training examples\n",
    "    y_mat = oneHotEncoding(y) #integer class coding into a one-hot representation\n",
    "    z = np.dot(x,w) #Then we compute raw class scores given our input and current weights\n",
    "    \n",
    "    #print(z[10000])\n",
    "    \n",
    "    prob = softmax(z+bias) #perform a softmax to get their probabilities\n",
    "    \n",
    "    #print(prob.shape)\n",
    "    loss = (-1 / m) * np.sum(y_mat * np.log(prob)) + (lamda/2)*np.sum(w*w) #We then find the loss of the probabilities\n",
    "    grad = (-1 / m) * np.dot(x.T,(y_mat - prob)) + lamda*w #compute the gradient for that loss\n",
    "    return loss,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "def getProbsAndPreds(data):\n",
    "    probs = softmax(np.dot(data,w))\n",
    "    preds = np.argmax(probs,axis=1)\n",
    "    return probs,preds\n",
    "\n",
    "print(train_data.shape[1])\n",
    "print(len(np.unique(train_target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.520448063638319\n"
     ]
    }
   ],
   "source": [
    "#Mini-batch stochastic gradient descent\n",
    "batch_size=100\n",
    "no_of_batches=500\n",
    "w = np.zeros([train_data.shape[1],len(np.unique(train_target))])\n",
    "b = np.zeros([batch_size,len(np.unique(train_target))])\n",
    "train_data_batched=np.split(train_data, no_of_batches)\n",
    "train_target_batched=np.split(train_target, no_of_batches)\n",
    "lamda = 1#10\n",
    "epochs = 300#\n",
    "learningRate = 1e-6\n",
    "losses = []\n",
    "for i in range(0,epochs):\n",
    "    for j in range(0,no_of_batches):\n",
    "        train_data_b=train_data_batched[j]\n",
    "        train_target_b=train_target_batched[j]\n",
    "        loss,grad = getGradientandLoss(w,train_data_b,train_target_b,lamda,b)\n",
    "        losses.append(loss)\n",
    "        w = w - (learningRate * grad)\n",
    "    \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def getAccuracy(data,target):\n",
    "    prob,prede = getProbsAndPreds(data)\n",
    "    \n",
    "    \n",
    "    accuracy = sum(prede == target)/(float(len(target)))\n",
    "    return prede,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmaxMNISTPred,softmaxMNISTAcc = getAccuracy(test_data,test_target)\n",
    "softmaxUSPSPred,softmaxUSPSAcc = getAccuracy(USPS_data,USPS_Target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def genMNISTModel(RawData):\n",
    "    input_size = RawData.shape[1]\n",
    "    drop_out = 0.2\n",
    "    first_dense_layer_nodes  = 128 \n",
    "    second_dense_layer_nodes = 64\n",
    "    \n",
    "    third_dense_layer_nodes = 10\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(first_dense_layer_nodes, input_dim=input_size))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(drop_out))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(Dense(second_dense_layer_nodes, input_dim=input_size))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(drop_out))\n",
    "\n",
    "    model.add(Dense(third_dense_layer_nodes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "MNISTmodel=genMNISTModel(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.4591 - acc: 0.8620 - val_loss: 0.1716 - val_acc: 0.9497\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.1999 - acc: 0.9401 - val_loss: 0.1209 - val_acc: 0.9648\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 24s 485us/step - loss: 0.1521 - acc: 0.9548 - val_loss: 0.0992 - val_acc: 0.9707\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 28s 557us/step - loss: 0.1233 - acc: 0.9625 - val_loss: 0.0903 - val_acc: 0.9724\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.1073 - acc: 0.9676 - val_loss: 0.0793 - val_acc: 0.9760\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.0933 - acc: 0.9710 - val_loss: 0.0872 - val_acc: 0.9735\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 0.0889 - acc: 0.9727 - val_loss: 0.0797 - val_acc: 0.9761\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 22s 449us/step - loss: 0.0785 - acc: 0.9752 - val_loss: 0.0769 - val_acc: 0.9765\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 21s 410us/step - loss: 0.0739 - acc: 0.9766 - val_loss: 0.0790 - val_acc: 0.9768\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.0679 - acc: 0.9789 - val_loss: 0.0785 - val_acc: 0.9774\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 26s 527us/step - loss: 0.0597 - acc: 0.9808 - val_loss: 0.0734 - val_acc: 0.9791\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.0596 - acc: 0.9803 - val_loss: 0.0787 - val_acc: 0.9778\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 21s 426us/step - loss: 0.0541 - acc: 0.9826 - val_loss: 0.0769 - val_acc: 0.9774\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0510 - acc: 0.9836 - val_loss: 0.0742 - val_acc: 0.9778\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 26s 521us/step - loss: 0.0484 - acc: 0.9840 - val_loss: 0.0855 - val_acc: 0.9771\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 22s 431us/step - loss: 0.0492 - acc: 0.9845 - val_loss: 0.0792 - val_acc: 0.9791\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 21s 422us/step - loss: 0.0458 - acc: 0.9847 - val_loss: 0.0780 - val_acc: 0.9780\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 20s 407us/step - loss: 0.0412 - acc: 0.9866 - val_loss: 0.0744 - val_acc: 0.9800\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.0406 - acc: 0.9867 - val_loss: 0.0769 - val_acc: 0.9800\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 25s 503us/step - loss: 0.0390 - acc: 0.9869 - val_loss: 0.0853 - val_acc: 0.9790\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 21s 426us/step - loss: 0.0416 - acc: 0.9863 - val_loss: 0.0788 - val_acc: 0.9787\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 21s 425us/step - loss: 0.0378 - acc: 0.9876 - val_loss: 0.0800 - val_acc: 0.9811\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 28s 562us/step - loss: 0.0362 - acc: 0.9876 - val_loss: 0.0766 - val_acc: 0.9790\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 21s 427us/step - loss: 0.0364 - acc: 0.9882 - val_loss: 0.0808 - val_acc: 0.9791\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 22s 434us/step - loss: 0.0349 - acc: 0.9886 - val_loss: 0.0782 - val_acc: 0.9803\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 22s 438us/step - loss: 0.0314 - acc: 0.9898 - val_loss: 0.0837 - val_acc: 0.9798\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 22s 441us/step - loss: 0.0321 - acc: 0.9891 - val_loss: 0.0840 - val_acc: 0.9800\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 21s 427us/step - loss: 0.0304 - acc: 0.9891 - val_loss: 0.0812 - val_acc: 0.9792\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0302 - acc: 0.9901 - val_loss: 0.0868 - val_acc: 0.9793\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 22s 436us/step - loss: 0.0283 - acc: 0.9912 - val_loss: 0.0844 - val_acc: 0.9794\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 22s 448us/step - loss: 0.0296 - acc: 0.9903 - val_loss: 0.0855 - val_acc: 0.9807\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 21s 420us/step - loss: 0.0298 - acc: 0.9906 - val_loss: 0.0925 - val_acc: 0.9799\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 0.0301 - acc: 0.9900 - val_loss: 0.0891 - val_acc: 0.9793\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 30s 601us/step - loss: 0.0271 - acc: 0.9909 - val_loss: 0.0906 - val_acc: 0.9804\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 0.0273 - acc: 0.9907 - val_loss: 0.0928 - val_acc: 0.9796\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 0.0267 - acc: 0.9907 - val_loss: 0.0915 - val_acc: 0.9807\n",
      "Epoch 00036: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "def runMNISTModel(train_data,train_target,model):\n",
    "    \n",
    "    validation_data_split = 0.2 # This is the validation data split ratio from the training set .This means last 20% of\n",
    "# data will be used as the validation set.\n",
    "    num_epochs = 50 # epochs are the number of datasets given to the model to learn.Changing to 1000 for 3 dense layers\n",
    "# as accuracy is reaching 0.99 in the first 200 epochs only   \n",
    "    model_batch_size = 100 # No of training examples used per iteration\n",
    "    tb_batch_size = 32 # batch_size used in callback functions to get the statistics of the model that is getting trained\n",
    "    early_patience = 25 # no of epochs with no improvement after which training is stopped\n",
    "\n",
    "# EarlyStopping is used to stop the training when a monitored value stops improving.\n",
    "    tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "    earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min')\n",
    "\n",
    "    TrainingData = train_data\n",
    "    TrainingTarget = train_target\n",
    "    TargetVector = np_utils.to_categorical(TrainingTarget,10)\n",
    "    #print(TrainingData)\n",
    "    history = model.fit(TrainingData\n",
    "                    , TargetVector\n",
    "                    , validation_data=(valid_data,np_utils.to_categorical(valid_target,10))\n",
    "                    , epochs=num_epochs\n",
    "                    , batch_size=model_batch_size\n",
    "                    , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                   )\n",
    "    return history\n",
    "\n",
    "history=runMNISTModel(train_data,train_target,MNISTmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistNNPredictions = np.argmax(MNISTmodel.predict(test_data),axis=1)\n",
    "uspsNNPredictions = np.argmax(MNISTmodel.predict(USPS_data),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 6 7 ... 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "# SVM CLassifier Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=200, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.05, kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "classifier = svm.SVC(C=200,kernel='linear',gamma=0.05)\n",
    "classifier.fit(train_data,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_pred_svm = classifier.predict(test_data)\n",
    "usps_pred_svm = classifier.predict(USPS_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "randomforestClassify = RandomForestClassifier(n_jobs=-1, n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomforestClassify.fit(train_data,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_pred_forest = randomforestClassify.predict(test_data)\n",
    "usps_pred_forest = randomforestClassify.predict(USPS_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
